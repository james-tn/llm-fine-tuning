$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline
experiment_name: sql_expert_pipeline
description: SQL Expert fine-tuning and evaluation pipeline-Phi4
inputs:
    model_dir:
        path: azureml://registries/azureml/models/Phi-4/versions/2
    dataset_name: sql_result_train.jsonl
    test_dataset: sql_result_test.jsonl
    db_path: northwind.db
outputs:
    trained_model:
        type: uri_folder
        mode: rw_mount
    evaluation_results:
jobs:
    train:
        type: command
        code: ./
        command: > 
            accelerate launch  
            --num_processes 1 
            --num_machines 1 
            --machine_rank $NODE_RANK 
            --main_process_ip $MASTER_ADDR 
            --main_process_port $MASTER_PORT 
            train.py 
            --model_name_or_path ${{inputs.model_dir}} 
            --dataset_name ${{inputs.dataset_name}} 
            --output_dir ${{outputs.trained_model}} 
            --num_train_epochs 3
            --max_seq_length 2048
            --learning_rate 1e-4
            --use_peft_lora True
            --seed 100  
            --sql_dataset_path sql_results.jsonl
            --include_assistant_reasoning False
            --chat_template_format "chatml"  
            --add_special_tokens False  
            --append_concat_token False  
            --splits "train,test"  
            --logging_steps 5  
            --log_level "info"  
            --logging_strategy "steps"  
            --eval_strategy "epoch"  
            --save_strategy "epoch"  
            --bf16 True  
            --packing True  
            --lr_scheduler_type "cosine"  
            --weight_decay 1e-4  
            --warmup_ratio 0.0  
            --max_grad_norm 1.0  
            --per_device_train_batch_size 3  
            --per_device_eval_batch_size 3 
            --gradient_accumulation_steps 8  
            --gradient_checkpointing True  
            --use_reentrant False  
            --dataset_text_field "content"  
            --lora_r 256  
            --lora_alpha 16  
            --lora_dropout 0.1  
            --lora_target_modules "all-linear"  
            --use_4bit_quantization False  
            --use_nested_quant False  
            --bnb_4bit_compute_dtype "bfloat16"  
            --bnb_4bit_quant_storage_dtype "bfloat16"
            --use_flash_attn True  
        inputs:
            model_dir: ${{parent.inputs.model_dir}}
            dataset_name: ${{parent.inputs.dataset_name}}
        outputs:
            trained_model: ${{parent.outputs.trained_model}}
        environment: 
            build:
                path: ./docker-context
        compute: azureml:nc40h100
        distribution:  
            type: pytorch  
            process_count_per_instance: 1 
        resources:  
            instance_count: 1  
    evaluate:
        type: command
        code: ./
        command: > 
            python evaluate.py 
            --model_dir ${{inputs.trained_model}} 
            --test_dataset ${{inputs.test_dataset}} 
            --db_path ${{inputs.db_path}} 
            --results_path ${{outputs.evaluation_results}}
        inputs:
            trained_model: ${{parent.jobs.train.outputs.trained_model}}
            test_dataset: ${{parent.inputs.test_dataset}}
            db_path: ${{parent.inputs.db_path}}
        outputs:
            evaluation_results: ${{parent.outputs.evaluation_results}}
        environment: 
            build:
                path: ./docker-context
        compute: azureml:nc40h100
