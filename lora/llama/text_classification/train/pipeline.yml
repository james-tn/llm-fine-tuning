$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline

experiment_name: llma3-classification
description: llma3-classification pipeline
inputs:
  num_examples: 56000 #set max number of examples to use for fine tuning
  model_name: llama3_classification #update this to the name of the fine tuned model
  mounted_data_file:
    mode: ro_mount
    path: azureml:training_data_jsonl_2024-10-25_154156_UTC:1
    type: uri_file
  model_dir: # path to the base model in registry.
    path: azureml://registries/azureml-meta/models/Llama-3.2-3B/versions/2
  epochs: 2
outputs:
  # map the output of the fine tuning job to the output of pipeline job so that we can easily register the fine tuned model
  # registering the model is required to deploy the model to an online or batch endpoint
  trained_model:
jobs:
  train:
    type: command
    code: ./
    command: >-
      python main.py
      --model_dir ${{inputs.model_dir}}
      --epochs ${{inputs.epochs}}
      --num_examples ${{inputs.num_examples}}
      --trained_model ${{outputs.output_dir}}
      --model_name ${{inputs.model_name}}
      --mounted_data_file ${{inputs.mounted_data_file}}
    inputs:
      epochs: ${{parent.inputs.epochs}}
      num_examples: ${{parent.inputs.num_examples}}
      model_dir: ${{parent.inputs.model_dir}} 
      model_name: ${{parent.inputs.model_name}}
      mounted_data_file: ${{parent.inputs.mounted_data_file}}
    environment: 
      conda_file: ./conda.yml
      image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1
    outputs:
      output_dir: ${{parent.outputs.trained_model}}
    compute: azureml:NC48adsA100 #update NC48adsA100 to the name of your compute target 

    distribution:
      type: pytorch
      process_count_per_instance: 2 #update this to the number of GPUs on your compute target
    resources:
      instance_count: 1 #update this to the number of nodes in your cluster
