$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json  
type: pipeline  
experiment_name: hscode_llama318b 
description: hscode_llama318b
inputs:  
  model_name: hscode_llama318b
  chat_model: False
  deepspeed: True  
  train_batch_size: 5  
  val_batch_size: 5  
  test_batch_size: 5  
  lora_r: 256  
  lora_alpha: 16  
  lora_dropout: 0.1  
  use_4bit: True  
  bnb_4bit_compute_dtype: float16  
  bnb_4bit_quant_type: nf4  
  use_nested_quant: False  
  fp16: False  
  bf16: True  
  gradient_accumulation_steps: 4  
  gradient_checkpointing: True  
  max_seq_length: 1024  
  packing: False  
  max_grad_norm: 0.3  
  learning_rate: 2e-4  
  weight_decay: 0.001  
  epochs: 3
  max_steps: 10
  train_dataset:  
    mode: ro_mount  
    path: azureml:hscode_train_short_ds:4
    type: uri_file  
  val_dataset:  
    mode: ro_mount  
    path: azureml:hscode_val_short_ds:4
    type: uri_file  
  test_dataset:  
    mode: ro_mount  
    path: azureml:hscode_test_short_ds:4
    type: uri_file  
  model_dir:  
    path: azureml://registries/azureml-meta/models/Meta-Llama-3.1-8B/versions/4
outputs:  
  trained_model:  
  ckp_dir:
  evaluated_model:  
jobs:  
  train:  
    type: command  
    code: ./  
    command: >-  
      python fine_tune.py  
      --model_dir ${{inputs.model_dir}}  
      --ckp_dir ${{outputs.ckp_dir}}  
      --epochs ${{inputs.epochs}}  
      --max_steps ${{inputs.max_steps}}  
      --trained_model ${{outputs.output_dir}}  
      --chat_model ${{inputs.chat_model}} 
      --deepspeed ${{inputs.deepspeed}}   
      --train_dataset ${{inputs.train_dataset}}  
      --val_dataset ${{inputs.val_dataset}}  
      --train_batch_size ${{inputs.train_batch_size}}  
      --val_batch_size ${{inputs.val_batch_size}}  
      --lora_r ${{inputs.lora_r}}  
      --lora_alpha ${{inputs.lora_alpha}}  
      --lora_dropout ${{inputs.lora_dropout}}  
      --use_4bit ${{inputs.use_4bit}}  
      --bnb_4bit_compute_dtype ${{inputs.bnb_4bit_compute_dtype}}  
      --bnb_4bit_quant_type ${{inputs.bnb_4bit_quant_type}}  
      --use_nested_quant ${{inputs.use_nested_quant}}  
      --fp16 ${{inputs.fp16}}  
      --bf16 ${{inputs.bf16}}  
      --gradient_accumulation_steps ${{inputs.gradient_accumulation_steps}}  
      --gradient_checkpointing ${{inputs.gradient_checkpointing}}  
      --max_seq_length ${{inputs.max_seq_length}}  
      --packing ${{inputs.packing}}  
      --max_grad_norm ${{inputs.max_grad_norm}}  
      --learning_rate ${{inputs.learning_rate}}  
      --weight_decay ${{inputs.weight_decay}}  
    inputs:  
      epochs: ${{parent.inputs.epochs}}  
      max_steps: ${{parent.inputs.max_steps}}  
      model_dir: ${{parent.inputs.model_dir}}  
      chat_model: ${{parent.inputs.chat_model}} 
      deepspeed: ${{parent.inputs.deepspeed}}   
      train_dataset: ${{parent.inputs.train_dataset}}  
      val_dataset: ${{parent.inputs.val_dataset}}  
      train_batch_size: ${{parent.inputs.train_batch_size}}  
      val_batch_size: ${{parent.inputs.val_batch_size}}  
      lora_r: ${{parent.inputs.lora_r}}  
      lora_alpha: ${{parent.inputs.lora_alpha}}  
      lora_dropout: ${{parent.inputs.lora_dropout}}  
      use_4bit: ${{parent.inputs.use_4bit}}  
      bnb_4bit_compute_dtype: ${{parent.inputs.bnb_4bit_compute_dtype}}  
      bnb_4bit_quant_type: ${{parent.inputs.bnb_4bit_quant_type}}  
      use_nested_quant: ${{parent.inputs.use_nested_quant}}  
      fp16: ${{parent.inputs.fp16}}  
      bf16: ${{parent.inputs.bf16}}  
      gradient_accumulation_steps: ${{parent.inputs.gradient_accumulation_steps}}  
      gradient_checkpointing: ${{parent.inputs.gradient_checkpointing}}  
      max_seq_length: ${{parent.inputs.max_seq_length}}  
      packing: ${{parent.inputs.packing}}  
      max_grad_norm: ${{parent.inputs.max_grad_norm}}  
      learning_rate: ${{parent.inputs.learning_rate}}  
      weight_decay: ${{parent.inputs.weight_decay}}  
    environment:  
      conda_file: ./conda.yml  
      image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1  
    outputs:  
      output_dir: ${{parent.outputs.trained_model}}  
      ckp_dir: ${{parent.outputs.ckp_dir}}  
    compute: azureml:NC48adsA100  
    distribution:  
      type: pytorch  
      process_count_per_instance: 2  
    resources:  
      instance_count: 1  
  evaluate:  
    type: command  
    code: ./  
    command: >-  
      python evaluate.py  
      --trained_model ${{inputs.trained_model}}  
      --model_name ${{inputs.model_name}}  
      --chat_model ${{inputs.chat_model}}  
      --test_dataset ${{inputs.test_dataset}}  
      --model_dir ${{inputs.model_dir}}
      --evaluated_model ${{outputs.evaluated_model}}  
    inputs:  
      trained_model: ${{parent.jobs.train.outputs.output_dir}}  
      model_name: ${{parent.inputs.model_name}}  
      test_dataset: ${{parent.inputs.test_dataset}}  
      chat_model: ${{parent.inputs.chat_model}}  
      model_dir: ${{parent.inputs.model_dir}}  
    outputs:  
      evaluated_model: ${{parent.outputs.evaluated_model}}  
    environment:  
      conda_file: ./conda.yml  
      image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1  
    compute: azureml:NC48adsA100  