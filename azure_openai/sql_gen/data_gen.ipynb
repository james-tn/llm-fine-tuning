{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "from pathlib import Path  \n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, stop_after_delay  \n",
    "from openai import AzureOpenAI  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from concurrent.futures import ThreadPoolExecutor  \n",
    "  \n",
    "def generate_data(MAX_REC_NUM=80, version_num=6):  \n",
    "    # Load environment variables  \n",
    "    env_path = Path('..') / 'secrets.env'  \n",
    "    load_dotenv(dotenv_path=env_path)  \n",
    "      \n",
    "    openaikey = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "    openaiservice = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "      \n",
    "    # Initialize OpenAI client  \n",
    "    client = AzureOpenAI(api_key=openaikey, api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"), azure_endpoint=openaiservice)  \n",
    "      \n",
    "    @retry(wait=wait_random_exponential(multiplier=1, max=60), stop=(stop_after_attempt(20) | stop_after_delay(300)))  \n",
    "    def generate_business_questions_and_queries(schema, prompt_type=\"single_scenario\"):  \n",
    "        user_message = f\"\"\"  \n",
    "        Use the provided database schema and business metrics definitions, generate at least 20 diverse and creative business questions, ranging from easy to advanced levels. Each question should be paired with a corresponding SQL query that answers it.  \n",
    "        ## Database Schema and Business Metrics Definitions  \n",
    "        {schema}  \n",
    "        ## Instructions:  \n",
    "        - Develop a variety of business questions to cover different aspects of the database.  \n",
    "        - Incorporate JOIN and aggregate functions to enhance the complexity and interest of the questions.  \n",
    "        - Ensure that the questions and queries are practical and relevant to real business problems.  \n",
    "        - Do not number the questions.  \n",
    "        ## Output Format:  \n",
    "        Output the questions and SQL queries in the following JSON format:  \n",
    "        {{  \n",
    "            \"questions\": [\"question1\", \"question2\", ...],  \n",
    "            \"sql_queries\": [\"query for question1\", \"query for question2\", ...],  \n",
    "            \"difficulty\": [\"easy\", \"medium\", \"advanced\", ...]  \n",
    "        }}  \n",
    "        \"\"\"  \n",
    "        if prompt_type == \"cross_scenario\":  \n",
    "            user_message = f\"\"\"  \n",
    "            Use the provided combined database schema and business metrics definitions from multiple scenarios, generate at least 10 diverse and creative business questions, ranging from easy to advanced levels. Each question should be paired with a corresponding SQL query that answers it.  \n",
    "            ## Database Schema and Business Metrics Definitions  \n",
    "            {schema}  \n",
    "            ## Instructions:  \n",
    "            - Develop a variety of business questions to cover different aspects of the database.  \n",
    "            - Ensure that each question represents more than one scenario (cross scenario question)  \n",
    "            - Incorporate JOIN and aggregate functions to enhance the complexity and interest of the questions.  \n",
    "            - Ensure that the questions and queries are practical and relevant to real business problems.  \n",
    "            - Do not number the questions.  \n",
    "            ## Output Format:  \n",
    "            Output the questions and SQL queries in the following JSON format:  \n",
    "            {{  \n",
    "                \"questions\": [\"question1\", \"question2\", ...],  \n",
    "                \"sql_queries\": [\"query for question1\", \"query for question2\", ...],  \n",
    "                \"difficulty\": [\"easy\", \"medium\", \"advanced\", ...]  \n",
    "            }}  \n",
    "            \"\"\"  \n",
    "          \n",
    "        response = client.chat.completions.create(  \n",
    "            model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),  \n",
    "            messages=[  \n",
    "                {\"role\": \"system\", \"content\": \"You are a smart AI assistant, you have excellent data analysis and SQL skills. You use SQL ANSI standard\"},  \n",
    "                {\"role\": \"user\", \"content\": user_message},  \n",
    "            ],  \n",
    "            response_format={\"type\": \"json_object\"},  \n",
    "            timeout=90,  \n",
    "        )  \n",
    "          \n",
    "        response_message = json.loads(response.choices[0].message.content)  \n",
    "        assert \"questions\" in response_message and \"sql_queries\" in response_message and \"difficulty\" in response_message  \n",
    "        return response_message  \n",
    "      \n",
    "    @retry(wait=wait_random_exponential(multiplier=1, max=60), stop=(stop_after_attempt(20) | stop_after_delay(300)))  \n",
    "    def review_questions_and_queries(schema, questions_and_queries):  \n",
    "        reviewed_questions_and_queries = {  \n",
    "            \"questions\": [],  \n",
    "            \"sql_queries\": [],  \n",
    "            \"reviews\": []  \n",
    "        }  \n",
    "          \n",
    "        for question, query in zip(questions_and_queries[\"questions\"], questions_and_queries[\"sql_queries\"]):  \n",
    "            user_message = f\"\"\"  \n",
    "            Given the following schema and business metrics definitions, and the following business question and corresponding SQL query, evaluate the correctness of the SQL query for the question. If there are any mistakes, provide the correct SQL query.  \n",
    "            ## Database Schema and Business Metrics Definitions  \n",
    "            {schema}  \n",
    "            ## Business Question and SQL Query  \n",
    "            Question: {question}  \n",
    "            SQL Query: {query}  \n",
    "            Output format: You write the evaluated question and sql query into json format as {{\"question\": \"question\", \"sql_query\": \"query\", \"review\": \"review\"}}.  \n",
    "            Your output:  \n",
    "            \"\"\"  \n",
    "            response = client.chat.completions.create(  \n",
    "                model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),  \n",
    "                messages=[  \n",
    "                    {\"role\": \"system\", \"content\": \"You are a SQL expert and you help review and correct SQL queries written by someone else. You are applying ANSI SQL standard\"},  \n",
    "                    {\"role\": \"user\", \"content\": user_message},  \n",
    "                ],  \n",
    "                response_format={\"type\": \"json_object\"},  \n",
    "            )  \n",
    "            response_message = json.loads(response.choices[0].message.content)  \n",
    "            assert \"question\" in response_message and \"sql_query\" in response_message and \"review\" in response_message  \n",
    "            reviewed_questions_and_queries[\"questions\"].append(response_message[\"question\"])  \n",
    "            reviewed_questions_and_queries[\"sql_queries\"].append(response_message[\"sql_query\"])  \n",
    "            reviewed_questions_and_queries[\"reviews\"].append(response_message[\"review\"])  \n",
    "          \n",
    "        return reviewed_questions_and_queries  \n",
    "      \n",
    "    def deduplicate_questions_and_queries(data):  \n",
    "        unique_questions = {}  \n",
    "        for question, query, review, difficulty in zip(data[\"questions\"], data[\"sql_queries\"], data.get(\"reviews\", []), data[\"difficulty\"]):  \n",
    "            if question not in unique_questions:  \n",
    "                unique_questions[question] = (query, review, difficulty)  \n",
    "        deduplicated_data = {  \n",
    "            \"questions\": list(unique_questions.keys()),  \n",
    "            \"sql_queries\": [unique_questions[q][0] for q in unique_questions],  \n",
    "            \"reviews\": [unique_questions[q][1] for q in unique_questions],  \n",
    "            \"difficulty\": [unique_questions[q][2] for q in unique_questions]  \n",
    "        }  \n",
    "        return deduplicated_data  \n",
    "  \n",
    "    def process_scenario(scenario_name, scenario_data):  \n",
    "        schema = json.dumps(scenario_data, indent=4)  \n",
    "        print(\"Working on scenario:\", scenario_name)  \n",
    "        final_data = {  \n",
    "            \"questions\": [],  \n",
    "            \"sql_queries\": [],  \n",
    "            \"reviews\": [],  \n",
    "            \"difficulty\": []  \n",
    "        }  \n",
    "          \n",
    "        while len(final_data[\"questions\"]) < MAX_REC_NUM:  \n",
    "            generated_data = generate_business_questions_and_queries(schema)  \n",
    "            reviewed_batch = review_questions_and_queries(schema, generated_data)  \n",
    "            final_data[\"questions\"].extend(reviewed_batch[\"questions\"])  \n",
    "            final_data[\"sql_queries\"].extend(reviewed_batch[\"sql_queries\"])  \n",
    "            final_data[\"reviews\"].extend(reviewed_batch[\"reviews\"])  \n",
    "            final_data[\"difficulty\"].extend(generated_data[\"difficulty\"])  \n",
    "            final_data = deduplicate_questions_and_queries(final_data)  \n",
    "          \n",
    "        if len(final_data[\"questions\"]) > MAX_REC_NUM:  \n",
    "            final_data[\"questions\"] = final_data[\"questions\"][:MAX_REC_NUM]  \n",
    "            final_data[\"sql_queries\"] = final_data[\"sql_queries\"][:MAX_REC_NUM]  \n",
    "            final_data[\"reviews\"] = final_data[\"reviews\"][:MAX_REC_NUM]  \n",
    "            final_data[\"difficulty\"] = final_data[\"difficulty\"][:MAX_REC_NUM]  \n",
    "          \n",
    "        return scenario_name, final_data  \n",
    "  \n",
    "    # Load existing data  \n",
    "    try:  \n",
    "        with open(\"./data/all_scenarios_questions_and_queries_v{version_num}.json\", \"r\") as file:  \n",
    "            existing_data = json.load(file)  \n",
    "    except FileNotFoundError:  \n",
    "        existing_data = {}  \n",
    "  \n",
    "    with open(\"./data/analytic_graph.json\", \"r\") as file:  \n",
    "        ontology = json.load(file)  \n",
    "      \n",
    "    split_scenarios = {}  \n",
    "    for scenario in ontology[\"business_scenarios\"]:  \n",
    "        scenario_name = scenario[\"scenario\"]  \n",
    "        split_scenarios[scenario_name] = {  \n",
    "            \"date_format\": ontology[\"date_format\"],  \n",
    "            \"scenario\": scenario,  \n",
    "            \"tables\": {},  \n",
    "            \"metrics\": []  \n",
    "        }  \n",
    "        for mapping in ontology[\"scenario_metric_mapping\"]:  \n",
    "            if mapping[\"scenario\"] == scenario_name:  \n",
    "                for metric_name in mapping[\"metrics\"]:  \n",
    "                    for metric in ontology[\"business_metrics\"]:  \n",
    "                        if metric[\"name\"] == metric_name:  \n",
    "                            split_scenarios[scenario_name][\"metrics\"].append(metric)  \n",
    "                            for table in metric[\"tables\"]:  \n",
    "                                if table not in split_scenarios[scenario_name][\"tables\"]:  \n",
    "                                    split_scenarios[scenario_name][\"tables\"][table] = ontology[\"tables\"][table]  \n",
    "      \n",
    "    split_scenarios[\"cross_scenario\"] = json.dumps(ontology, indent=4)  \n",
    "    all_scenarios_output = {}  \n",
    "  \n",
    "    with ThreadPoolExecutor() as executor:  \n",
    "        results = executor.map(lambda item: process_scenario(*item), split_scenarios.items())  \n",
    "        for scenario_name, final_data in results:  \n",
    "            print(\"new data created len \", len(final_data))\n",
    "            \n",
    "            # Merge with existing data  \n",
    "            if scenario_name in existing_data.keys():  \n",
    "                existing_data[scenario_name][\"questions\"].extend(final_data[\"questions\"])  \n",
    "                existing_data[scenario_name][\"sql_queries\"].extend(final_data[\"sql_queries\"])  \n",
    "                existing_data[scenario_name][\"reviews\"].extend(final_data[\"reviews\"])  \n",
    "                existing_data[scenario_name][\"difficulty\"].extend(final_data[\"difficulty\"])  \n",
    "                print(\"data merged len before dedupe \", len(existing_data))\n",
    "                existing_data[scenario_name] = deduplicate_questions_and_queries(existing_data[scenario_name])  \n",
    "                print(\"data merged len after dedupe \", len(existing_data[scenario_name]))\n",
    "\n",
    "            else:  \n",
    "                print(\"scenario not in existing data \", scenario_name)\n",
    "                print(\"existing data \", existing_data.keys())\n",
    "                existing_data[scenario_name] = final_data  \n",
    "  \n",
    "    all_scenarios_output = existing_data  \n",
    "  \n",
    "    # Save merged and deduplicated data  \n",
    "    try:  \n",
    "        with open(f\"./data/all_scenarios_questions_and_queries_v{version_num}.json\", \"w\") as file:  \n",
    "            json.dump(all_scenarios_output, file, indent=4)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to all_scenarios_questions_and_queries_v{version_num}.json: {e}\")  \n",
    "  \n",
    "    all_data = []  \n",
    "    for scenario_name, reviewed_data in all_scenarios_output.items():  \n",
    "        for question, query, difficulty in zip(reviewed_data[\"questions\"], reviewed_data[\"sql_queries\"], reviewed_data[\"difficulty\"]):  \n",
    "            all_data.append({  \n",
    "                \"scenario\": scenario_name,  \n",
    "                \"input\": question,  \n",
    "                \"output\": query,  \n",
    "                \"difficulty\": difficulty  \n",
    "            })  \n",
    "  \n",
    "    train, test = train_test_split(all_data, test_size=0.2, stratify=[item[\"scenario\"] for item in all_data])  \n",
    "  \n",
    "    def create_message_format(item, schema, include_context=True):  \n",
    "        schema_context = f\"## Database Schema and Business Metrics Definitions\\n{schema}\\n\\n## Question: \" if include_context else \"\"  \n",
    "        user_content = f\"{schema_context}{item['input']}\"  \n",
    "        return {  \n",
    "            \"messages\": [  \n",
    "                {\"role\": \"system\", \"content\": \"You are a smart AI assistant with excellent SQL and data analysis skills. You are querying the MDDX database, what is the SQL query for the following question?\"},  \n",
    "                {\"role\": \"user\", \"content\": user_content},  \n",
    "                {\"role\": \"assistant\", \"content\": item[\"output\"]}  \n",
    "            ]  \n",
    "        }  \n",
    "  \n",
    "    try:  \n",
    "        with open(f\"./data/train_data_v{version_num}.json\", \"w\") as f:  \n",
    "            json.dump(train, f, indent=4)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to train_data_v{version_num}.json: {e}\")  \n",
    "  \n",
    "    try:  \n",
    "        with open(f\"./data/test_data_v{version_num}.json\", \"w\") as f:  \n",
    "            json.dump(test, f, indent=4)  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to test_data_v{version_num}.json: {e}\")  \n",
    "  \n",
    "    print(f\"Train with {len(train)} and test data saved to ./data/train_data_v{version_num}.json and ./data/test_data_v{version_num}.json\")  \n",
    "  \n",
    "    try:  \n",
    "        with open(f\"./data/sqltrain_openai_ctx_v{version_num}.jsonl\", \"w\") as f:  \n",
    "            for item in train:  \n",
    "                schema = json.dumps(split_scenarios[item[\"scenario\"]], indent=4) if item[\"scenario\"] != \"cross_scenario\" else split_scenarios[\"cross_scenario\"]  \n",
    "                f.write(json.dumps(create_message_format(item, schema)) + \"\\n\")  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to sqltrain_openai_ctx_v{version_num}.jsonl: {e}\")  \n",
    "  \n",
    "    try:  \n",
    "        with open(f\"./data/sqltest_openai_ctx_v{version_num}.jsonl\", \"w\") as f:  \n",
    "            for item in test:  \n",
    "                schema = json.dumps(split_scenarios[item[\"scenario\"]], indent=4) if item[\"scenario\"] != \"cross_scenario\" else split_scenarios[\"cross_scenario\"]  \n",
    "                f.write(json.dumps(create_message_format(item, schema)) + \"\\n\")  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to sqltest_openai_ctx_v{version_num}.jsonl: {e}\")  \n",
    "  \n",
    "    try:  \n",
    "        with open(f\"./data/sqltrain_openai_v{version_num}.jsonl\", \"w\") as f:  \n",
    "            for item in train:  \n",
    "                schema = json.dumps(split_scenarios[item[\"scenario\"]], indent=4) if item[\"scenario\"] != \"cross_scenario\" else split_scenarios[\"cross_scenario\"]  \n",
    "                f.write(json.dumps(create_message_format(item, schema, False)) + \"\\n\")  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to sqltrain_openai_v{version_num}.jsonl: {e}\")  \n",
    "  \n",
    "    try:  \n",
    "        with open(f\"./data/sqltest_openai_v{version_num}.jsonl\", \"w\") as f:  \n",
    "            for item in test:  \n",
    "                schema = json.dumps(split_scenarios[item[\"scenario\"]], indent=4) if item[\"scenario\"] != \"cross_scenario\" else split_scenarios[\"cross_scenario\"]  \n",
    "                f.write(json.dumps(create_message_format(item, schema, False)) + \"\\n\")  \n",
    "    except Exception as e:  \n",
    "        print(f\"Error writing to sqltest_openai_v{version_num}.jsonl: {e}\")  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on scenario: Order Analysis\n",
      "Working on scenario: Customer Analysis\n",
      "Working on scenario: Product Analysis\n",
      "Working on scenario: Employee Performance\n",
      "Working on scenario: Supplier Analysis\n",
      "Working on scenario: Shipping Analysis\n",
      "Working on scenario: Inventory Management\n",
      "Working on scenario: cross_scenario\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    generate_data(version_num=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
