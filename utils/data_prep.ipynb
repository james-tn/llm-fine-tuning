{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from pathlib import Path  \n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "env_path = Path('.') / 'secrets.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "openai.api_key =  os.environ.get(\"AZURE_OPENAI_API_KEY\")\n",
    "openai.api_base =  os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = \"2023-07-01-preview\"\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, stop_after_delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\")\n",
    "dataset_splits = {\"train\": dataset[\"train\"]}\n",
    "\n",
    "# os.makedirs(\"../data/\", exist_ok=True)\n",
    "# out_path=\"../data/sql-create-context.jsonl\"\n",
    "# for key, ds in dataset_splits.items():\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         for item in ds:\n",
    "#             newitem = {\n",
    "#                 \"input\": item[\"question\"],\n",
    "#                 \"context\": item[\"context\"],\n",
    "#                 \"output\": item[\"answer\"],\n",
    "#             }\n",
    "#             f.write(json.dumps(newitem) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "all_data = pd.read_json(\"../data/sql-create-context.jsonl\", lines=True)\n",
    "# train_data, test_data = train_test_split(all_data, test_size=0.8)\n",
    "all_data.head()\n",
    "#extract table names from context and add to a new column table_names\n",
    "all_data['table_names'] = all_data['context'].str.findall(r'(?<=\\bCREATE TABLE\\s)\\w+')\n",
    "#extract top 20 most frequent table names\n",
    "top_20_tables = all_data['table_names'].explode().value_counts()[:20].index.tolist()\n",
    "#filter data to only include rows with top 20 table names\n",
    "all_data = all_data[all_data['table_names'].apply(lambda x: any([item in x for item in top_20_tables]))]\n",
    "train_data, test_data = train_test_split(all_data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.to_json(\"../llama2/data/sql-create-context-train.jsonl\", orient=\"records\", lines=True)\n",
    "test_data.to_json(\"../llama2/data/sql-create-context-test.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_data_dict = pd.read_json(\"../llama2/data/sql-create-context-train.jsonl\", lines=True).to_dict(orient=\"records\")\n",
    "inputs = [item[\"input\"] for item in list_data_dict]\n",
    "outputs = [item[\"output\"] for item in list_data_dict]\n",
    "\n",
    "# dataset = Dataset.from_dict({\"input\": inputs, \"output\":outputs})   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation (Skip this step if data is already generated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "table: Categories, columns: CategoryID INTEGER, CategoryName TEXT, Description TEXT, Picture BLOB \n",
    "table: CustomerDemographics, columns: CustomerTypeID TEXT, CustomerDesc TEXT \n",
    "table: Customers, columns: CustomerID TEXT, CompanyName TEXT, ContactName TEXT, ContactTitle TEXT, Address TEXT, City TEXT, Region TEXT, PostalCode TEXT, Country TEXT, Phone TEXT, Fax TEXT \n",
    "table: Employees, columns: EmployeeID INTEGER, LastName TEXT, FirstName TEXT, Title TEXT, TitleOfCourtesy TEXT, BirthDate DATE, HireDate DATE, Address TEXT, City TEXT, Region TEXT, PostalCode TEXT, Country TEXT, HomePhone TEXT, Extension TEXT, Photo BLOB, Notes TEXT, ReportsTo INTEGER, PhotoPath TEXT \n",
    "table: EmployeeTerritories, columns: EmployeeID INTEGER, TerritoryID TEXT \n",
    "table: [Order Details], columns: OrderID INTEGER, ProductID INTEGER, UnitPrice NUMERIC, Quantity INTEGER, Discount REAL \n",
    "table: Orders, columns: OrderID INTEGER, CustomerID TEXT, EmployeeID INTEGER, OrderDate DATETIME, RequiredDate DATETIME, ShippedDate DATETIME, ShipVia INTEGER, Freight NUMERIC, ShipName TEXT, ShipAddress TEXT, ShipCity TEXT, ShipRegion TEXT, ShipPostalCode TEXT, ShipCountry TEXT \n",
    "table: Products, columns: ProductID INTEGER, ProductName TEXT, SupplierID INTEGER, CategoryID INTEGER, QuantityPerUnit TEXT, UnitPrice NUMERIC, UnitsInStock INTEGER, UnitsOnOrder INTEGER, ReorderLevel INTEGER, Discontinued TEXT \n",
    "table: Regions, columns: RegionID INTEGER, RegionDescription TEXT table: Shippers, columns: ShipperID INTEGER, CompanyName TEXT, Phone TEXT table: Suppliers, columns: SupplierID INTEGER, CompanyName TEXT, ContactName TEXT, ContactTitle TEXT, Address TEXT, City TEXT, Region TEXT, PostalCode TEXT, Country TEXT, Phone TEXT, Fax TEXT, HomePage TEXT table: Territories, columns: TerritoryID TEXT, TerritoryDescription TEXT, RegionID INTEGER\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema.strip()\n",
    "schema = schema.split(\"\\n\")\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\n",
    "\n",
    "# @retry(stop=(stop_after_delay(1) | stop_after_attempt(5)))\n",
    "def generate_reading_comp_question(schema):\n",
    "#     user_message =f\"\"\" \n",
    "#      You are training new employees to use the database. Given the following schema, generate at least 150 questions and answers that guide them to memorize the tables' schemas to write SQL query for business.\n",
    "#      Try to give the business context to the question.\n",
    "#         {schema}\n",
    "#     You write the question and answer into multi-line json format as {{\"question\": \"Some question\", \"answer\": \"Some answer\"}}.\n",
    "#     Your output:\n",
    "# \"\"\"\n",
    "    user_message =f\"\"\" \n",
    "     You are training new employees to use the database. Given the following schema, generate at least 50 business questions and corresponding SQL query that can give answer to the question.\n",
    "     <<Database schema>>\n",
    "        {schema}\n",
    "    Try to be creative in the business problem. Use join and aggregate functions to make the question more interesting.\n",
    "    Output format: You write the question and sql query into multi-line json format as {{\"question\": \"Some question\", \"sql_query\": \"Some answer\"}}.\n",
    "    Your output:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data analyst working with a database\"},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# schema_comp = []\n",
    "# for table in schema:\n",
    "#     table_output = generate_reading_comp_question(table)\n",
    "#     print(table_output)\n",
    "#     schema_comp.append(table_output)\n",
    "    \n",
    "# schema_comp\n",
    "outputs = []\n",
    "for i in range(10):\n",
    "    output = generate_reading_comp_question(schema)\n",
    "    outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_df = pd.read_json(\"../llama2/data/sql_examples.jsonl\", lines=True)\n",
    "out_df.to_json(\"../llama2/data/sql_examples.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=\"\"\n",
    "#write code to remove all the numbered items like 1. 2. from the output\n",
    "for output in outputs:\n",
    "    out= re.sub(r'\\d+\\.\\s+', '', outputs[1])\n",
    "    result += out\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
