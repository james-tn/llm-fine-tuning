$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json  
type: pipeline  
experiment_name: hscode_llama3-70b_LORA  
description: hscode_llama3-70b_LORA 
inputs:  
  model_name: hscode_llama3-70b_LORA  
  chat_model: False  
  enable_deepspeed: True  
  max_steps: 100  
  train_batch_size: 1  
  val_batch_size: 1  
  test_batch_size: 5  
  lora_r: 256  
  lora_alpha: 16  
  lora_dropout: 0.05  
  use_lora: True  # Added use_lora parameter  
  deepspeed_config: deepspeed_configs/ds_config.json
  target_modules: q_proj o_proj k_proj v_proj gate_proj up_proj down_proj
  enable_quantization: True  
  use_4bit_quantization: True  
  bnb_4bit_quant_storage_dtype: uint8
  bnb_4bit_compute_dtype: bfloat16  
  bnb_4bit_quant_type: nf4  
  use_nested_quant: False  
  fp16: False  
  bf16: True  
  gradient_accumulation_steps: 4  
  gradient_checkpointing: True  
  use_flash_attn: True  
  max_seq_length: 1024  
  packing: False  
  max_grad_norm: 0.3  
  learning_rate: 2e-4  
  weight_decay: 0.001  
  epochs: 3  
  verbosity: INFO  # Added verbosity parameter  
  train_dataset:  
    mode: ro_mount  
    path: azureml:hscode_train_short_ds:4  
    type: uri_file  
  val_dataset:  
    mode: ro_mount  
    path: azureml:hscode_val_short_ds:4  
    type: uri_file  
  test_dataset:  
    mode: ro_mount  
    path: azureml:hscode_test_short_ds:4  
    type: uri_file  
  model_dir:  
    # path: azureml://registries/azureml-meta/models/Meta-Llama-3.1-8B/versions/4
    # path: azureml://registries/azureml-meta/models/Meta-Llama-3-70B/versions/7  
    # path: azureml://registries/azureml/models/mistralai-Mixtral-8x7B-v01/versions/15
    path: azureml://registries/azureml-meta/models/Meta-Llama-3.1-70B/versions/4
outputs:  
  trained_model:  
  ckp_dir:  
  evaluated_model:  
jobs:  
  train:  
    type: command  
    code: ./  
    command: >  
      python fine_tune.py  
      --model_dir ${{inputs.model_dir}}  
      --ckp_dir ${{outputs.ckp_dir}}  
      --epochs ${{inputs.epochs}}  
      --max_steps ${{inputs.max_steps}}  
      --trained_model ${{outputs.output_dir}}  
      --use_flash_attn ${{inputs.use_flash_attn}}   
      --chat_model ${{inputs.chat_model}}  
      --enable_deepspeed ${{inputs.enable_deepspeed}}  
      --enable_quantization ${{inputs.enable_quantization}}  
      --bnb_4bit_quant_storage_dtype ${{inputs.bnb_4bit_quant_storage_dtype}}  
      --use_4bit_quantization ${{inputs.use_4bit_quantization}}
      --train_dataset ${{inputs.train_dataset}}  
      --val_dataset ${{inputs.val_dataset}}  
      --train_batch_size ${{inputs.train_batch_size}}  
      --val_batch_size ${{inputs.val_batch_size}}  
      --lora_r ${{inputs.lora_r}}  
      --lora_alpha ${{inputs.lora_alpha}}  
      --lora_dropout ${{inputs.lora_dropout}}  
      --use_lora ${{inputs.use_lora}}  # Pass use_lora to the script  
      --bnb_4bit_compute_dtype ${{inputs.bnb_4bit_compute_dtype}}  
      --bnb_4bit_quant_type ${{inputs.bnb_4bit_quant_type}}  
      --use_nested_quant ${{inputs.use_nested_quant}}  
      --fp16 ${{inputs.fp16}}  
      --bf16 ${{inputs.bf16}}  
      --gradient_accumulation_steps ${{inputs.gradient_accumulation_steps}}  
      --gradient_checkpointing ${{inputs.gradient_checkpointing}}  
      --max_seq_length ${{inputs.max_seq_length}}  
      --packing ${{inputs.packing}}  
      --max_grad_norm ${{inputs.max_grad_norm}}  
      --learning_rate ${{inputs.learning_rate}}  
      --weight_decay ${{inputs.weight_decay}}  
      --deepspeed_config ${{inputs.deepspeed_config}}  
      --target_modules ${{inputs.target_modules}}  
      --verbosity ${{inputs.verbosity}}  # Pass verbosity to the script  
    inputs:  
      epochs: ${{parent.inputs.epochs}}  
      max_steps: ${{parent.inputs.max_steps}}  
      model_dir: ${{parent.inputs.model_dir}}  
      chat_model: ${{parent.inputs.chat_model}}  
      use_flash_attn: ${{parent.inputs.use_flash_attn}}  
      enable_deepspeed: ${{parent.inputs.enable_deepspeed}}  
      train_dataset: ${{parent.inputs.train_dataset}}  
      val_dataset: ${{parent.inputs.val_dataset}}  
      train_batch_size: ${{parent.inputs.train_batch_size}}  
      val_batch_size: ${{parent.inputs.val_batch_size}}  
      lora_r: ${{parent.inputs.lora_r}}  
      lora_alpha: ${{parent.inputs.lora_alpha}}  
      lora_dropout: ${{parent.inputs.lora_dropout}}  
      use_lora: ${{parent.inputs.use_lora}}  # Ensure use_lora is included  
      use_4bit_quantization: ${{parent.inputs.use_4bit_quantization}}  
      bnb_4bit_compute_dtype: ${{parent.inputs.bnb_4bit_compute_dtype}}  
      bnb_4bit_quant_type: ${{parent.inputs.bnb_4bit_quant_type}}  
      use_nested_quant: ${{parent.inputs.use_nested_quant}}  
      bnb_4bit_quant_storage_dtype: ${{parent.inputs.bnb_4bit_quant_storage_dtype}}
      fp16: ${{parent.inputs.fp16}}  
      bf16: ${{parent.inputs.bf16}}  
      gradient_accumulation_steps: ${{parent.inputs.gradient_accumulation_steps}}  
      gradient_checkpointing: ${{parent.inputs.gradient_checkpointing}}  
      max_seq_length: ${{parent.inputs.max_seq_length}}  
      packing: ${{parent.inputs.packing}}  
      max_grad_norm: ${{parent.inputs.max_grad_norm}}  
      learning_rate: ${{parent.inputs.learning_rate}}  
      weight_decay: ${{parent.inputs.weight_decay}}  
      deepspeed_config: ${{parent.inputs.deepspeed_config}}  
      target_modules: ${{parent.inputs.target_modules}}  
      enable_quantization: ${{parent.inputs.enable_quantization}}  
      verbosity: ${{parent.inputs.verbosity}}  
    environment:  
      conda_file: ./conda.yml  
      image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1  
    outputs:  
      output_dir: ${{parent.outputs.trained_model}}  
      ckp_dir: ${{parent.outputs.ckp_dir}}  
    compute: azureml:nc80h100  
    distribution:  
      type: pytorch  
      process_count_per_instance: 2 
    resources:  
      instance_count: 1  
  
  evaluate:  
    type: command  
    code: ./  
    command: >  
      python evaluate.py  
      --trained_model ${{inputs.trained_model}}  
      --model_name ${{inputs.model_name}}  
      --chat_model ${{inputs.chat_model}}  
      --test_dataset ${{inputs.test_dataset}}  
      --model_dir ${{inputs.model_dir}}  
      --evaluated_model ${{outputs.evaluated_model}}  
      --use_lora ${{inputs.use_lora}}  
      --verbosity ${{inputs.verbosity}}  # Pass verbosity to the evaluation script  
    inputs:  
      trained_model: ${{parent.jobs.train.outputs.output_dir}}  
      model_name: ${{parent.inputs.model_name}}  
      test_dataset: ${{parent.inputs.test_dataset}}  
      chat_model: ${{parent.inputs.chat_model}}  
      model_dir: ${{parent.inputs.model_dir}}  
      use_lora: ${{parent.inputs.use_lora}}  
      verbosity: ${{parent.inputs.verbosity}}  
    outputs:  
      evaluated_model: ${{parent.outputs.evaluated_model}}  
    environment:  
      conda_file: ./conda.yml  
      image: mcr.microsoft.com/azureml/curated/acpt-pytorch-2.2-cuda12.1  
    compute: azureml:nc80h100  