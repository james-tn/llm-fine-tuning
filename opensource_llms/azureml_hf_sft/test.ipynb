{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After training the model, we can use this notebook to test the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Direct download the model and test. Run this notebook in A100 GPU machine (NC24adsA100 compute instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "import time\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "credential = InteractiveBrowserCredential()\n",
    "subscription_id = \"\" # your subscription id\n",
    "resource_group = \"\"#your resource group\n",
    "workspace = \"\" #your workspace name\n",
    "workspace_ml_client = MLClient(credential, subscription_id, resource_group, workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For regular model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llama2_13b_fine_tuned\"\n",
    "model_path=\"./\"\n",
    "workspace_ml_client.models.download(model_name, version=\"2\",download_path=model_path)\n",
    "#after this step, remove the redundant parent folder name \"llama2_13b_fine_tuned\" so that the downloaded folder only has one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "example = {\"context\":\"You are querying the sales database, what is the SQL query for the following question?\",\"input\":\"What is the total revenue for each territory?\"}\n",
    "PROMPT_DICT =\"\\n{context}\\n\\n### Question:\\n{input}\\n\\n### Response:{output}\"\n",
    "PROMPT_DICT_CHAT =\"<s>[INST]\\n{context}\\n\\n### Question:\\n{input}\\n[/INST]\"\n",
    "model = mlflow.pyfunc.load_model(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For regular model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "prompt = PROMPT_DICT.format(input=example[\"input\"], context=example[\"context\"])\n",
    "prompt = {\"role\": \"user\",\"content\": prompt} \n",
    "model.predict([prompt])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "prompt = PROMPT_DICT_CHAT.format(input=example[\"input\"], context=example[\"context\"])\n",
    "prompt = {\"role\": \"user\",\"content\": prompt} \n",
    "model.predict([prompt])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Deploy to managed online endpoint and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create online endpoint: ```az ml online-endpoint create -f deployment/endpoint.yml```\n",
    "2. Create the deployment: ```az ml online-deployment update -f deployment/deployment.yml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"output\": \"#\\\\n### Response:SELECT Suppliers.SupplierID, AVG(Products.UnitPrice) AS AverageUnitPrice FROM Products INNER JOIN Suppliers ON Products.SupplierID = Suppliers.SupplierID GROUP BY Suppliers.SupplierID\\\\n###\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\"}'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "question = \"What is the average unit price of products by each supplier?\"\n",
    "\n",
    "# content = \"Hi there\"\n",
    "\n",
    "data= {\"data\":{\"text\":[question], \"max_gen_len\":100, \"temperature\":0.9}}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://llma2-fine-tuning.westus2.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
    "api_key = ''\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deploy to AKS and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create AKS cluster\n",
    "az aks create -g ml -n aksgpu2 --enable-managed-identity --node-count 1 --enable-addons monitoring --generate-ssh-keys --node-vm-size standard_nc24ads_a100_v4\n",
    "\n",
    "#Install k8s-extension\n",
    "az k8s-extension create --name ml --extension-type Microsoft.AzureML.Kubernetes --config enableTraining=True enableInference=True inferenceRouterServiceType=LoadBalancer allowInsecureConnections=True InferenceRouterHA=False --cluster-type managedClusters --cluster-name aksgpu2 --resource-group ml --scope cluster\n",
    "\n",
    "#Install Nvidia extension\n",
    "az aks get-credentials --resource-group ml --name aksgpu2\n",
    "\n",
    "kubectl apply -f nvidia_device.yaml\n",
    "\n",
    "#create namespace\n",
    "kubectl create namespace gpu-resources\n",
    "\n",
    "#create instance type\n",
    "kubectl apply -f instance_type.yaml\n",
    "\n",
    "###az aks nodepool add --resource-group ml --cluster-name aks001 --name gpunp --node-count 1 --node-vm-size standard_nc24ads_a100_v4 --node-taints sku=gpu:NoSchedule --aks-custom-headers UseGPUDedicatedVHD=true --enable-cluster-autoscaler --min-count 1 --max-count 3\n",
    "\n",
    "#attach to azure ml workspace\n",
    "\n",
    "az ml compute attach --resource-group ml --workspace-name ws01ent --type Kubernetes --name aksgpu2 --resource-id \"/subscriptions/840b5c5c-3f4a-459a-94fc-6bad2a969f9d/resourcegroups/ml/providers/Microsoft.ContainerService/managedClusters/aksgpu2\" --identity-type SystemAssigned --no-wait --namespace gpu-resources\n",
    "\n",
    "#create the online endpoint\n",
    "az ml online-endpoint create -f k8s_endpoint.yml\n",
    "#create the deployment\n",
    "az ml online-deployment create -f k8s_deployment.yml\n",
    "\n",
    "\n",
    "\n",
    "#Delete deployments in case needed\n",
    "az ml online-deployment delete --name blue --endpoint-name llm-k8s-gpu --yes --resource-group ml --workspace-name ws01ent\n",
    "az ml online-deployment delete --name blue --endpoint-name llm-k8s-ep --yes --resource-group ml --workspace-name ws01ent\n",
    "az ml online-deployment delete --name green --endpoint-name llm-k8s-ep --yes --resource-group ml --workspace-name ws01ent\n",
    "\n",
    "az ml online-endpoint delete --name ws01ent-bajsw --resource-group ml --workspace-name ws01ent --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "prompt = \"\"\"\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Summarize the following input to less than 30 words .\n",
    "### Input:\n",
    "In general, perplexity is a measurement of how well a probability model predicts a sample. In the context of Natural Language Processing, perplexity is one way to evaluate language models.\n",
    "A language model is a probability distribution over sentences: it’s both able to generate plausible human-written sentences (if it’s a good language model) and to evaluate the goodness of already written sentences. Presented with a well-written document, a good language model should be able to give it a higher probability than a badly written document, i.e. it should not be “perplexed” when presented with a well-written document.\n",
    "Thus, the perplexity metric in NLP is a way to capture the degree of ‘uncertainty’ a model has in predicting (i.e. assigning probabilities to) text.\"\"\"\n",
    "\n",
    "instruction =\"You are querying the sales database, what is the SQL query for the following input question?\"\n",
    "input = \"What is the average unit price of products by each supplier?\"\n",
    "content = f\"<s>[INST]\\n{instruction}\\n\\n### Input:\\n{input}\\n[/INST]\"\n",
    "\n",
    "# content = \"Hi there\"\n",
    "\n",
    "data= {\"data\":{\"text\":content, \"max_length\":100}}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'http://20.72.223.233/api/v1/endpoint/llm-k8s-gpu/score'\n",
    "api_key= ''\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'blue' }\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
